---
title: "Loading and processing logbook data"
author: "Genoveva, Kotaro Ono"
---


# Introduction

This file loads and processes the logbook data and links it with the sales note to have a data ready to be further processed to a variety of data-call that require such data.
It only works for 2015-present data as a consequence. 

Here are the general steps of the data processing: 
1. Because the logbook is only for NORWEGIAN vessels, we filter out all NON-NORWEGIAN sales note.
2. Because the logbook requirement was to vessels >15m until end of 2020, >11m from July 1st 2022, >10m from July 1st 2023, >8m from January 1st 2025, we filter these out too in order to efficiently link the two data soruces
3. There are some cases where the sales note does not record information in radiocallsignal. In that case, the link is made to the logbook via the registration number
4. then we proceed with the data processing while having to make the assumptions below.

DISCLAIMER:
There are several caveats/assumptions that are made during the data processing that need to be aware of:
1. the information on target species is not directly available in the logbook data (present in the DEP but no longer present together with DCA since 2015). Therefore, I used the HOVEDART as a replacement. This should have minor influence on the metier 6 level (though not verified)
2. there are many cases (13% (2015), 0 (2016), 13% (2017), 12% (2018), 13% (2019), 13% (2020), 10% (2021)) where the RC is not available in the sales note. This creates a problem for merging the logbook data with the sales note (only place where the sell value is available).
3. the merging between logbook and salesnote happens by searching, for a specific RC and species, the nearest date between the salesnote reporting and the end of fishing activity (based on logbook stopdate). there is a histogram that is outputted in the section [weight difference between reported caught and reported landed] which shows the discrepancy and there is a lot. But nothing can be done at the moment... this is a task that is difficult to make automatic BECAUSE some people keep fish in a pond and sell later or sells at different times, etc)
4. Relatedly, there are cases where we do not have an associated sell price info on the species for a specific logbook entry (because cannot be linked to sales note). This can be as much as 27% (2015), 29% (2016) 23% (2017), 28% (2018), 3% (2019), 17% (2020), 2% 2021),  of the logbook data. In these cases, the average price per species for that year is used to calculate the sell value of the logbook catch.
5. some species are excluded from the data from the get-go e.g. calanus, whale, 
6. some issues with the coordinates are fixed but those that are non-obvious (thus no correct ices area) are removed
and there are a total 7487 (2015), 6905 (2020), 8768 (2021)    of these weird points
7. we only keep data within the major fishing area 27
8. we only keep the logbook data with valid haul (no gear problem) and where species are identified i.e. different from an empty field and when the boat is "actively fishing". 



# Prep
## Generic stuff
```{r, warning=FALSE}
DataPath <- "Q:\\ressurs\\mare\\fiskstat\\dagbok\\elFangstdagbok_detaljert_FerdigeÅrEtter2011"

# Loading libraries and reading in functions

library(chron)
library(plyr)
library(lubridate)
library(data.table)
library(purrr)
library(parapurrr) # do purrr in parallel multicore
library(stringr)
library(dplyr)
library(sp)
library(sf)
library(ggplot2)
library(rgdal)
library(survival) # neardate
library(readr) # read_delim
library(openxlsx) # read.xlsx
library(tidyr)

for(f in list.files(path=paste0(getwd(), "/Functions"), full.names = T)){
  source(f)
}

# Selecting the year
year <- 2015
# year <- 2016
# year <- 2017
# year <- 2018
# year <- 2019
# year <- 2020
# year <- 2021
# year <- 2022


```

## Import the ices area shapefile
```{r}
library(sf)
new_proj <- 3035
ICES <- read_sf("D:/Dropbox/IMR_projects/Shapefiles/ICES_Areas_20160601_cut_dense_3857.shp")
world <- st_read("D:/Dropbox/IMR_projects/Shapefiles/ne_10m_land.shp")
world <- world %>% st_make_valid()
Atlantic <- st_crop(world, c(xmin = -60, ymin = 45, xmax = 32, ymax = 80))
Atlantic_proj <- st_transform(Atlantic, crs=new_proj)

ICES_df <- ICES %>% st_drop_geometry()
ICES_sf <- ICES %>% st_transform(new_proj)
ggplot(ICES) + geom_sf() + geom_sf_label(aes(label =  Area_Full))

```


## Get logbook data
```{r, read_logbook, echo=TRUE}
# data from from Daily Catch Report (DCA) from the Q drive

if (year < 2022) {
  file_list <- list.files("Q:\\ressurs\\mare\\fiskstat\\dagbok\\elFangstdagbok_detaljert_FerdigeÅrEtter2011", full.names = TRUE)
  file_list <- file_list[grep("FDIR", file_list)]
  file_list <- file_list[grep("psv", file_list)]
  fil <- grep(paste0(year, "_"), file_list)
}

if (year == 2022) {
  file_list = c("Q:\\ressurs\\mare\\fiskstat\\dagbok\\elFangstdagbok_detaljert\\2022\\12_2022\\FDIR_HI_ERS_2022_PR_2022-12-12.psv")
  fil = 1
}

## Using read.csv to enable norwegian character reading
d1 <- read.csv(file_list[fil],                       
                        dec = ",",
                        sep = "|",
                        #sep = ";",
                        #stringsAsFactors=F
                        fileEncoding = "iso-8859-1")
                        #fileEncoding="UTF-16LE")


## How many have a missing RC in the logbook? they are all norwegian vessels
sum(is.na(d1$RC))/nrow(d1)

```

## Get saleslip data (both catch and value)
```{r, read_salesnote, echo=TRUE}

file_list_fangst1 <- "Q:\\ressurs\\mare\\fiskstat\\sluttseddel\\sluttseddel_LSS_ferdigeÅr_fra2005\\2005-2015\\Fangst\\FDIR_HI_LSS_FANGST_2015_PR_2016-12-08.psv"
file_list_verdi1 <- "Q:\\ressurs\\mare\\fiskstat\\sluttseddel\\sluttseddel_LSS_ferdigeÅr_fra2005\\2005-2015\\Verdi\\FDIR_HI_LSS_VERDI_2015_PR_2016-12-08.psv"

fils <- list.files(paste0("Q:\\ressurs\\mare\\fiskstat\\sluttseddel\\sluttseddel_LSS_ferdigeÅr_fra2005\\", year), full.names = TRUE) 
file_list_fangst2 <- fils[grep("FANGST", fils)]
file_list_verdi2 <- fils[grep("VERDI", fils)]

if (year == 2015) {
  file_fangst = file_list_fangst1
  file_verdi = file_list_verdi1
}
if (year > 2015) {
  file_fangst = file_list_fangst2
  file_verdi = file_list_verdi2
}
if (year == 2022) {
  file_fangst = "Q:\\ressurs\\mare\\fiskstat\\sluttseddel\\sluttseddel_LSS\\mottatt2022\\12_2022\\FDIR_HI_LSS_FANGST_2022_PR_2022-12-05.psv"
  file_verdi = "Q:\\ressurs\\mare\\fiskstat\\sluttseddel\\testversjon_SSB\\mottatt_2022\\12_2022_Verdi_LSS\\FDIR_HI_LSS_VERDI_2022_PR_2022-12-05.psv"
}

fangst <- read.csv(file_fangst,                       
                        dec = ",",
                        sep = "|",
                        #sep = ";",
                        #stringsAsFactors=F
                        fileEncoding = "iso-8859-1")
                        #fileEncoding="UTF-16LE")
verdi <- read.csv(file_verdi,                       
                        dec = ",",
                        sep = "|",
                        #sep = ";",
                        #stringsAsFactors=F
                        fileEncoding = "iso-8859-1")
                        #fileEncoding="UTF-16LE")

## merge the two data 
  fangst_verdi <- merge(fangst, verdi)
# needcols <- c(1,30,31,33,50,58,77,78,79,89, 105,106,107,108,110,115)
# fangst_verdi <- fangst_verdi[,needcols]
  fangst_verdi$LDAT <- as.POSIXct(as.character(fangst_verdi$Landingsdato),format="%d.%m.%Y", tz="UTC")

## Filter out ALL non-norwegian vessels
  fangst_verdi <- fangst_verdi[which(fangst_verdi$Fartøynasjonalitet == "NORGE"),]

## Filter out based on the size of the vessel and the logbook requirement rule change
if (year < 2021) fangst_verdi <- fangst_verdi %>% filter(Største.lengde >= 15)
if (year >= 2021) {
  sel1 <- which(fangst_verdi$LDAT >= "2021-01-01 UTC" & fangst_verdi$LDAT < "2023-07-01 UTC") 
  sel2 <- which(fangst_verdi$LDAT >= "2023-07-01 UTC" & fangst_verdi$LDAT < "2025-01-01 UTC")
  fangst_verdi1 <- NULL
  fangst_verdi2 <- NULL
  if (length(sel1) >0) fangst_verdi1 <- fangst_verdi[sel1, ] %>% filter(Største.lengde >= 11)
  if (length(sel2) >0) fangst_verdi2 <- fangst_verdi[sel2, ] %>% filter(Største.lengde >= 10)
  fangst_verdi <- rbind(fangst_verdi1, fangst_verdi2)
}

## If the radiocall signal is missing, we can find the missing info in the logbook from the registration number
  missingRC <- unique(fangst_verdi[which(fangst_verdi$Radiokallesignal..seddel.==""),'Registreringsmerke..seddel.'])
  if (length(missingRC)>0){
    for (i in seq_along(missingRC)){
      val = missingRC[i]
      RC_found = unique(d1[which(d1$REGM == val),'RC'])
      if (length(RC_found)>1) break("Found multiple match!! Check the problem!")
      if (length(RC_found)==0) print("nothing found")
      if (length(RC_found)==1) fangst_verdi[which(fangst_verdi$Registreringsmerke..seddel. == val),'Radiokallesignal..seddel.'] = RC_found
    }
  }
  
## How many have a missing RC in the sales notes? 
  fangst_verdi$Radiokallesignal..seddel.[which(fangst_verdi$Radiokallesignal..seddel.=="")] <- NA
  Salesnote_missingRC <- sum(is.na(fangst_verdi$Radiokallesignal..seddel.))/nrow(fangst_verdi)
  Salesnote_missingRC_which <- which(is.na(fangst_verdi$Radiokallesignal..seddel.) == TRUE)
  print(Salesnote_missingRC)
  readline(prompt="Check the output. Then press [enter] to continue")

## Keep only info needed
  needcols <- c(1,30,31,50,58,77,78,79,89, 105,106,107,108,110,115,116)
  fangst_verdi <- fangst_verdi[,needcols]


rm(fangst, verdi)

```


## Re-format logbook data to resemble eflalo2
```{r, reformat_logbook, echo=TRUE}

d1$VE_REF <- d1$RC
d1$VE_FLT <- NA
d1$VE_COU <- "NO"
d1$VE_LEN <- d1$STØRSTE_LENGDE
d1$VE_KW <- d1$MOTORKRAFT/1.36
d1$VE_TON <- d1$BRUTTOTONNASJE

d1$FT_REF <- NA
d1$FT_DCOU <- "NO"
d1$FT_DHAR <- NA

d1$STARTTIDSPUNKT <- as.POSIXct(d1$STARTTIDSPUNKT,format="%Y-%m-%d %H:%M:%S")
d1$STOPPTIDSPUNKT <- as.POSIXct(d1$STOPPTIDSPUNKT,format="%Y-%m-%d %H:%M:%S")

# the following fields should contain FT info, but I will use them to store haul (incl. catch) data

d1$FT_DDAT <- lubridate::as_date(d1$STARTTIDSPUNKT)
d1$FT_DDAT <- format(d1$FT_DDAT, "%d/%m/%Y")

d1$FT_DTIME <- times(format(d1$STARTTIDSPUNKT, "%H:%M:%S"))

d1$FT_LCOU <- "NO"

d1$FT_LHAR <- NA

d1$FT_LDAT <- lubridate::as_date(d1$STOPPTIDSPUNKT)
d1$FT_LDAT <- format(d1$FT_LDAT, "%d/%m/%Y")

d1$FT_LTIME <- times(format(d1$STOPPTIDSPUNKT, "%H:%M:%S"))

d1$FT_REF <- paste(d1$RC,year(d1$STARTTIDSPUNKT), month(d1$STARTTIDSPUNKT),
                     day(d1$STARTTIDSPUNKT), hour(d1$STARTTIDSPUNKT),
                     minute(d1$STARTTIDSPUNKT),as.character(d1$VARIGHET),
                     sep = "-")

# approximate LE data follows

d1$LE_ID <- row.names(d1)
d1$LE_CDAT <- lubridate::as_date(d1$STARTTIDSPUNKT)
d1$LE_STIME <- times(format(d1$STARTTIDSPUNKT, "%H:%M:%S"))
d1$LE_ETIME <- times(format(d1$STOPPTIDSPUNKT, "%H:%M:%S"))
d1$LE_SLAT <- d1$START_LT
d1$LE_SLON <- d1$START_LG
d1$LE_ELAT <- d1$STOPP_LT
d1$LE_ELON <- d1$STOPP_LG
d1$LE_GEAR <- d1$REDSKAP_FAO
d1$LE_MSZ <- d1$MASKEVIDDE
d1$LE_RECT <- d1$INT_OMR_NY_STOPP # or should it be the rectangle at the end? or should i over the pol?
d1$LE_DIV <- NA
d1$LE_MET <- NA

head(d1)
```

## Check period
```{r}
min(d1$STARTTIDSPUNKT, na.rm=TRUE); max(d1$STARTTIDSPUNKT, na.rm=TRUE)
```

## How many NAs in the date field?
```{r}
length(sum(is.na(d1$STARTTIDSPUNKT)))/dim(d1)[1]
```

# Fill out metier field
## Transcode gear
```{r, add_metier, echo=TRUE}
d2 <- d1

idx1 <- which(d2$REDSKAP_FAO=="GEN")
d2$LE_GEAR[idx1] <-"GNS"

idx2<-which(d2$REDSKAP_FAO=="LL")
d2$LE_GEAR[idx2] <-"LLS"

idx3<-which(d2$REDSKAP_FAO%in%c("PS1", "PS2"))
d2$LE_GEAR[idx3] <-"PS"

idx4<-which(d2$REDSKAP_FAO%in%c("OTS", "TB", "TBN", "TBS"))
d2$LE_GEAR[idx4] <-"OTB"

idx5<-which(d2$REDSKAP_FAO%in%c("TM", "TMS"))
d2$LE_GEAR[idx5] <-"OTM"
```

## creating a new data.frame with area, metier info added
```{r, simple, echo=TRUE}
### Create input data
input.data <- data.table(Country = d2$VE_COU,
                         year = year(as.POSIXct(d2$FT_DDAT[1],format="%d/%m/%Y")),
                         vessel_id = d2$VE_REF,
                         vessel_length = d2$VE_LEN,
                         trip_id = d2$FT_REF,
                         haul_id = d2$LE_ID,
                         fishing_day = as.character(d2$LE_CDAT),
                         area = as.character(NA),
                         ices_rectangle = d2$LE_RECT,
                         gear = d2$LE_GEAR,
                         gear_FR = as.character(NA),
                         mesh = d2$LE_MSZ,
                         selection = "0",
                         registered_target_assemblage = as.character(NA),
                         FAO_species = as.character(NA),
                         metier_level_6 = as.character(NA),
                         measure =as.character(NA),
                         KG = as.numeric(NA),
                         EUR = as.numeric(NA))

validateInputDataFormat(input.data) # the input needs to be a data.table!

### Load reference lists
url <- "https://github.com/ices-eg/RCGs/raw/master/Metiers/Reference_lists/AreaRegionLookup.csv"
area.list <- loadAreaList(url)
url <- "https://github.com/ices-eg/RCGs/raw/master/Metiers/Reference_lists/Metier%20Subgroup%20Species%202020.xlsx"
species.list <- loadSpeciesList(url)
url <- "https://github.com/ices-eg/RCGs/raw/master/Metiers/Reference_lists/RDB_ISSG_Metier_list.csv"
metier.list <- loadMetierList(url)
url <- "https://github.com/ices-eg/RCGs/raw/master/Metiers/Reference_lists/Code-ERSGearType-v1.1.xlsx"
gear.list <- loadGearList(url)
assemblage.list <- unique(c(species.list$species_group, species.list$dws_group))
assemblage.list <- assemblage.list[!is.na(assemblage.list)]
rm(url)

### Input data codes validation
validateInputDataCodes(input.data, gear.list, area.list, species.list)

### Get FAO area code for each fishing operation & assign to the data
### Using the start point as the centroid sometimes gives a weird location
### doing this manually then using some info from the original data
### there are some weird data points that need attention 

  ## negative latitude (though could be krill... antartic krill)
    sum_weird  = 0
    sum_weird = sum_weird + length(which(d2$LE_SLAT < 0))
    # d2$LE_SLAT[which(d2$LE_SLAT < 0)] = - d2$LE_SLAT[which(d2$LE_SLAT < 0)]
    # d2$START_LT[which(d2$START_LT < 0)] = - d2$START_LT[which(d2$START_LT < 0)]
  
  ## using the value in the data if existing (either start or stop)
     d2$area = case_match(d2$INT_OMR_NY_START, "IIa2" ~ "27.2.a.2", "Ib" ~ "27.1.b", "IVa" ~ "27.4.a", "IIIa"~ "27.3.a", "VIa"~"27.6.a", "IVb"~"27.4.b",  "IIb2"~"27.2.b.2", "VIIk1"~"27.7.k.1", 
                          "VIIc2"~"27.7.c.2", "Va2"~"27.5.a.2",  "VIb1"~"27.6.b.1", "VIIb"~"27.7.b", "Vb1b"~"27.5.b.1.b", "VIIc1"~"27.7.c.1", "Ia"~"27.1.a", "XIVb2"~"27.14.b.2", "IVc"~"27.4.c", 
                          "XIVa"~"27.14.a", "Vb2"~"27.5.b.2", "IIa1"~"27.2.a.1", "IIb1"~"27.2.b.1", "VIIj2"~"27.7.j.2", "XIVb1"~"27.14.b.1", "VIb2"~"27.6.b.2", "3M"~"3M", "1D"~"1D")
  
     d2$area1 = case_match(d2$INT_OMR_NY_STOPP, "IIa2" ~ "27.2.a.2", "Ib" ~ "27.1.b", "IVa" ~ "27.4.a", "IIIa"~ "27.3.a", "VIa"~"27.6.a", "IVb"~"27.4.b",  "IIb2"~"27.2.b.2", "VIIk1"~"27.7.k.1", 
                          "VIIc2"~"27.7.c.2", "Va2"~"27.5.a.2",  "VIb1"~"27.6.b.1", "VIIb"~"27.7.b", "Vb1b"~"27.5.b.1.b", "VIIc1"~"27.7.c.1", "Ia"~"27.1.a", "XIVb2"~"27.14.b.2", "IVc"~"27.4.c", 
                          "XIVa"~"27.14.a", "Vb2"~"27.5.b.2", "IIa1"~"27.2.a.1", "IIb1"~"27.2.b.1", "VIIj2"~"27.7.j.2", "XIVb1"~"27.14.b.1", "VIb2"~"27.6.b.2", "3M"~"3M", "1D"~"1D")

  d2sp <- data.frame(lon=d2$LE_SLON, lat=d2$LE_SLAT)
  
  Data_sf <- d2sp %>% st_as_sf(crs = 4326, coords = c("lon", "lat")) %>% st_cast("POINT") %>% st_transform(new_proj)
  Area_intersect <- as.matrix(st_intersects(Data_sf, ICES_sf))
  which_area <- apply(Area_intersect, 1, function(x) ifelse(TRUE %in% x, which(x == TRUE), NA))
  input.data$area[!is.na(which_area)] <- ICES_df$Area_Full[which_area][!is.na(which_area)]
  input.data$area[is.na(input.data$area)] <- d2$area1[is.na(input.data$area)]
  
  ## Now replacing visually, the ones on land with the nearest ices area values
  # area 27.2.a.2
    d2$ID <- 1:nrow(d2)
    NAtosolve = which(is.na(input.data$area) == TRUE)
    which_2a2 <- d2[NAtosolve,] %>% filter(START_LT > 63, START_LT < 73, START_LG < 30, START_LG > 0) %>% select(ID)
    input.data$area[unlist(which_2a2)] = "27.2.a.2"
    sum_weird = sum_weird + length(which_2a2)
  # area 27.4.a
    which_4a<- d2[NAtosolve,] %>% filter(START_LT < 63, START_LT > 58, START_LG < 30, START_LG > 0) %>% select(ID)
    input.data$area[unlist(which_4a)] = "27.4.a"
    sum_weird = sum_weird + length(which_4a)
  
  # visually check what is going on:
    blabla <- d2[which(is.na(input.data$area) == TRUE),] %>% st_as_sf(crs = 4326, coords = c("LE_SLON", "LE_SLAT")) %>% st_transform(new_proj)
    blabla$X = st_coordinates(blabla)[,1]
    blabla$Y = st_coordinates(blabla)[,2]
    
    x11()
    ggplot(Atlantic) + geom_sf() + geom_point(data = blabla, aes(x=START_LG, y =START_LT), col="red") + 
      coord_sf(ylim = c(50, 80), xlim=c(-60, 40), expand = FALSE) + theme_bw()

    readline(prompt="Check the red dots on the map. Do we have missing ones around Norway?
             If satified press [enter] to continue")
    
    dev.off()
    
  ## Now only keeping the ones that falls within the Atlantic Northeast region (major fishing area 27)
    input.data$ID = 1:nrow(input.data)
    input.data.clean = input.data[grep("27", input.data$area)]
  
  ## Now clean all logbook input where fish were not caught due to some issues (gear problem, etc)
    input.data.clean = input.data.clean %>% filter(!ID %in% d2$ID[which(d2$FANGSTART_FAO == "")])
       
  ## Merge by area
    input.data.clean <- merge(input.data.clean, area.list, all.x = T, by = "area")

  ## Total number of "weird" data points that were both fixed and not fixed
    sum_weird = sum_weird + nrow(input.data) - nrow(input.data.clean)
    print(sum_weird)
    readline(prompt="Check the output. Then press [enter] to continue")

##### Fill in target species - but the intended species info disappeared from the IMR internal data after 2015
##### For the time being, we can use the HOVEDART instead. Need to check how different the metier result will be between the two methods
# depdata <- d1_1
# 
# depdata$STARTTIDSPUNKT <- as.POSIXct(depdata$STARTTIDSPUNKT,format="%d.%m.%Y %H:%M:%S")
# depdata$STOPPTIDSPUNKT <- as.POSIXct(depdata$STOPPTIDSPUNKT,format="%d.%m.%Y %H:%M:%S")
# 
# # if start point is null, use stop time date and set time to midnight
# 
# idx <- which(is.na(depdata$STARTTIDSPUNKT)&!is.na(depdata$STOPPTIDSPUNKT))
# depdata$STARTTIDSPUNKT[idx] <- paste(as.character(date(depdata$STOPPTIDSPUNKT[idx])),"00:00:00", sep = " ")
# depdata$STARTTIDSPUNKT[idx] <- as.POSIXct(depdata$STARTTIDSPUNKT[idx],format="%d.%m.%Y %H:%M:%S")
# depdata$FT_REF <- paste(depdata$RC,year(depdata$STARTTIDSPUNKT), month(depdata$STARTTIDSPUNKT),
#                      day(depdata$STARTTIDSPUNKT), hour(depdata$STARTTIDSPUNKT),
#                      minute(depdata$STARTTIDSPUNKT),as.character(depdata$VARIGHET),
#                      sep = "-")

## Removing a few species that are obviously not relevant (e.g. calanus, minky whale)
input.data.clean <- input.data.clean %>%
  mutate(target_species =  d2$HOVEDART_FAO[input.data.clean$ID] )

input.data.clean <- input.data.clean %>% filter(! target_species %in% c("JCM", "MIW"))

## Adding a few species not present
species.list = rbind(species.list, 
                     data.frame(FAO_species="ATG", species_group ="DEF", dws_group=NA), # polar cod
                     data.frame(FAO_species="GDG", species_group ="DEF", dws_group=NA), # norway pout
                     data.frame(FAO_species="KCD", species_group ="CRU", dws_group=NA)) # king crab

input.data.clean <- input.data.clean %>%
  mutate(registered_target_assemblage=species.list$species_group[match(target_species,species.list$FAO_species)])


# Assign gear group and re-coded gear name to the input data
input.data.clean <- merge(input.data.clean, gear.list, all.x = T, by.x = "gear", by.y = "gear_code")

input.data.clean$seq_dom_group<-NA
input.data.clean$selection_type<-NA
input.data.clean$selection_mesh<-NA

# input.data.clean[,c("metier_level_6","metier_level_5") <- pa_pmap_dfr(list(RCG,
#                                                                 year,
#                                                                 gear_level6, 
#                                                                 registered_target_assemblage,
#                                                                 seq_dom_group, 
#                                                                 mesh, 
#                                                                 selection_type,
#                                                                 selection_mesh
#                                                                 ), getMetier)]

input.data.clean[,c("metier_level_6","metier_level_5"):=pmap_dfr(list(RCG,
                                                                 year,
                                                                 gear_level6, 
                                                                 registered_target_assemblage,
                                                                 seq_dom_group, 
                                                                 mesh, 
                                                                 selection_type,
                                                                 selection_mesh
                                                                 ), getMetier)]

length(which(input.data.clean$metier_level_6=="MIS_MIS_0_0_0"))/dim(input.data.clean)[1]

rm(Data_sf, d2sp)

```


## Other fields
```{r}
d3 <- input.data.clean %>% left_join(d2, by = c("haul_id"="LE_ID"))

d3$MESHSIZE <- d3$MASKEVIDDE
d3$MESHCAT <- NA
#d3$INTV <- d3$VARIGHET

```


## sort out class and values in vessel id field
```{r, aggregate, echo=TRUE}
fangst_verdi$Landingsdato <- fangst_verdi$LDAT
if (year < 2018) fangst_verdi$landing_datetime <- with(fangst_verdi, paste(Landingsdato, 
                                                          substr(as.POSIXct(sprintf("%04.0f", Landingsklokkeslett), format='%H%M'), 12, 16), sep = " "))
if (year >= 2018) fangst_verdi$landing_datetime <- with(fangst_verdi, paste(Landingsdato, 
                                                          Landingsklokkeslett, sep = " "))
fangst_verdi$landing_datetime <- as.POSIXct(fangst_verdi$landing_datetime)
fangst_verdi$Radiokallesignal..seddel.[which(is.na(fangst_verdi$Radiokallesignal..seddel.))] <- "UNKNOWN"
fangst_verdi$ID <- as.numeric(rownames(fangst_verdi))

## aggregate by species and day/time: sum weight, mean unit price, sum value

fangst_verdi_agg <- fangst_verdi %>% group_by(landing_datetime, Radiokallesignal..seddel., Art.FAO..kode.) %>%
  dplyr::summarize(Bruttovekt=sum(Bruttovekt),
            Produktvekt=sum(Produktvekt),
            Rundvekt=sum(Rundvekt),
            Enhetspris.for.kjøper=mean(Enhetspris.for.kjøper),
            Enhetspris.for.fisker=mean(Enhetspris.for.fisker),
            Fangstverdi=sum(Fangstverdi))

rm(fangst_verdi, d1, d2)

```


## Connect logbook to landings (salesnote)
most likely selling unit price for the catch
```{r, add_monetary_value, echo=TRUE}
## Find the closest landing date (and time!) in sales data posterior to catch date (and time!) in logbook data (not working perfectly due to time being ignored by the neardate fun...)
d4<-d3

data1 <- d4 %>% select("RC", "STOPPTIDSPUNKT", "FANGSTART_FAO", "RUNDVEKT", "FT_REF")
data2 <- subset(fangst_verdi_agg, select = c("Art.FAO..kode.",
                                                        "Radiokallesignal..seddel.",
                                                        "landing_datetime",
                                                        "Produktvekt",
                                             "Enhetspris.for.fisker",
                                                        "Fangstverdi"))
#column numbers in 2019 are c(8,3,16,11,12,17)

data1$id <- paste(data1$RC,data1$FANGSTART_FAO, sep="")
data2$id <- paste(data2$Radiokallesignal..seddel., data2$Art.FAO..kode., sep = "")

data1 <-data1[order(data1$id,data1$STOPPTIDSPUNKT),]
data2 <-data2[order(data2$id,data2$landing_datetime),]

indx1 <- neardate(data1$id, data2$id, as.Date(data1$STOPPTIDSPUNKT), as.Date(data2$landing_datetime))
res <- cbind(data1, data.frame(data2[indx1,]))
colnames(res)[13]<-"id2"
res

## this is the number of records from the logbook that were not able to be "linked" to the sales note
sum(is.na(res$Radiokallesignal..seddel.))/nrow(res)

readline(prompt="Check the output. Then press [enter] to continue")

rm(d3)

```

### weight difference between reported caught and reported landed
the sum of the weights across hauls needs to be not more than x% different to the landed weight which is associated to all those hauls (repeat values)
here a "fake" percentage differnce is calculated to have a more robust measure
```{r, check_diff_logbook_salesnote, echo=TRUE}
w1 <- res %>%
  group_by(Art.FAO..kode., Radiokallesignal..seddel., landing_datetime) %>%
  dplyr::summarise(minweight=min(Produktvekt), maxweight = max(Produktvekt),
            #.groups = c("Art.FAO..kode.", "Radiokallesignal..seddel.", "landing_datetime")
            ) %>%
  mutate(check = minweight-maxweight)

w2 <- res %>%
  group_by(FANGSTART_FAO, Radiokallesignal..seddel., landing_datetime) %>%
  dplyr::summarize(sumweight=sum(RUNDVEKT), 
            #.groups = c("FANGSTART_FAO", "Radiokallesignal..seddel.", "landing_datetime")
            )

wc <- merge(w1,w2, by.x = c("Art.FAO..kode.", "Radiokallesignal..seddel.", "landing_datetime"), by.y=c("FANGSTART_FAO", "Radiokallesignal..seddel.", "landing_datetime"))

# the real percent difference is not calculated in order to avoid 0 value in the reference
# percentage_difference <- function(value, ref) {
#   (value - (ref+0.01)) / (ref+0.01) * 100
# } 
percentage_difference <- function(value, value_two) {
  (value - value_two) / ((value + value_two) / 2) * 100
} 
pct_diff <- percentage_difference(wc$maxweight, wc$sumweight)    # we are comparing how much landing is different from logbook. to correct logbook record afterwards 

hist(pct_diff)
```

### Redistribute landings over hauls
Equivalent to dispatch over pings. Could potentially use vmstools function
Two options: sum the value across landing times (just like i did with weights) and then correct *that* using the pct difference. Or correct the weight reported at catch time using the percent difference and multiply *that* times the mean unit price for that species at that landing event. 
correction: RUNDVEKT + (RUNDVEKT*(pct_diff/100))
Some duplicates are removed
```{r, adjust_logbook_to_salesnote, echo=TRUE}
## sum value over landing event

# is it worth doing this? I can't be bothered

## correct catch to match landing

res <- left_join(res, cbind(wc, pct_diff))

d5 <- left_join(d4, select(res, 
                           Enhetspris.for.fisker, 
                           FANGSTART_FAO, 
                           FT_REF, 
                           pct_diff, 
                           STOPPTIDSPUNKT, RUNDVEKT, RC )) 
d5 <- d5[!duplicated(d5)] %>% 
  mutate(RUNDVEKT1 = case_when(
    (pct_diff>-50 & pct_diff<50) ~ as.numeric(RUNDVEKT) + (as.numeric(RUNDVEKT)*(pct_diff/100.00))
    ,
    TRUE ~ as.numeric(RUNDVEKT)
    )
  )


```

### how much catch didn't get a value assigned?
```{r}
length(which(is.na(d5$Enhetspris.for.fisker)))/dim(d5)[1]

readline(prompt="Check the output. Then press [enter] to continue")

```

### for those where unit price is unknown, get average price for same year, and species
```{r, calc_price, echo=TRUE}
avpric<-fangst_verdi_agg %>%
  group_by(Art.FAO..kode.) %>%
  dplyr::summarize(av_price = mean(Enhetspris.for.fisker, na.rm=TRUE)) %>%
  filter(Art.FAO..kode.%in%unique(d5$FANGSTART_FAO))

names(avpric$av_price) <- avpric$Art.FAO..kode.

d5$unit_price <- NA

for(i in avpric$Art.FAO..kode.){
  idx<-which(is.na(d5$Enhetspris.for.fisker)&d5$FANGSTART_FAO==i)
  d5$unit_price[idx]<-avpric$av_price[i]
}

```

## Calculate value
```{r, add_value, echo=TRUE}
d5 <- d5 %>% mutate(value=case_when(
  !is.na(unit_price)~RUNDVEKT1*unit_price,
  !is.na(Enhetspris.for.fisker)~RUNDVEKT1*Enhetspris.for.fisker
)
)
```

## Get exchange rate
### Option 1, get the exchange rate for the actual sell day (overkill?)
```{r}
# # By default return the NOK to EUR rate for today's date 
# 
# f2 <- function(base = "NOK", to = "EUR", date = Sys.Date()) {
#     # Data from https://exchangeratesapi.io (sourced from European Central Bank)
#     if(length(date) > 1) {
#         x <- as.Date(date)
#         src <- URLencode(paste0("https://api.exchangeratesapi.io/history?start_at=", min(x, na.rm = TRUE), "&end_at=", max(x, na.rm = TRUE), "&base=",base))
#     } else {
#         src <- paste0("https://api.exchangeratesapi.io/",date,"?base=",base)
#     }
# 
#     parsed = tryCatch({
#         jsonlite::fromJSON(src)
#     }, error = function(e) {
#         return(NA)
#     })
# 
#     # Error checks
#     if(!is.list(parsed) || length(parsed$rates) < 1) {
#         warning("Error getting data! Check your input parameter(s).")
#         return(NA)
#     }
# 
#     if(length(date) > 1) {
#         tgt <- match(date, names(parsed$rates))
#         ret <- lapply(tgt, function(x) return(parsed$rates[[x]][[to]]))
#         ret <- suppressWarnings(as.numeric(as.character(ret)))
#     } else {
#         if(parsed$date != date)
#             warning(paste("Rate from", date , "is not available. Rate from", parsed$date, "is used instead."))
#         ret <- parsed$rates[[to]]
#     }
#     return(as.numeric(ret))
# }
# 
# f2(date = c("2020-01-31", "2020-02-31","2020-01-30", "2020-01-15"))
# f2(date = c("2020-01-31", "2030-02-31","2020-01-30", "2020-01-15"), base = "USD", to = "NOK")
# 
# 
# #for(i in 1:dim(d5)[1]){
# #  print(i)
# #  d5$exrate[i] <- f1(date = d5$Landingsdato[i])
# #} # this takes way too long
# 
# 
# 
# # make lookup table of dates and exchange rates
# 
# getDays <- function(year){
#      seq(as.Date(paste(year, "-01-01", sep="")), as.Date(paste(year, "-12-31", sep="")), by="+1 day")
# }
# 
# date_lookup <- data.frame(date=getDays(year), exrate = NA)
# 
# for(i in 1:dim(date_lookup)[1]){
#   print(i)
#   date_lookup$exrate[i] <- f1(date = date_lookup$date[i])
# } 
# 
# d5$exrate <- date_lookup$exrate[match(d5$Landingsdato,date_lookup$date)]

```

### Option 2, get the mean exchange rate of the last 10 years
```{r, exchange_rate, echo=TRUE}
exrate <-mean(
0.110552,
0.133735,
0.12822,
0.119702,
0.111943,
0.107605,
0.107233,
0.104102,
0.101541,
0.093316,
0.098127) # https://www.ofx.com/en-au/forex-news/historical-exchange-rates/yearly-average-rates/

d5 <- d5 %>% mutate(value_euro=value*exrate)

```


### percent withouth weight data
```{r}
nrow(d5[which(is.na(d5$RUNDVEKT)|d5$RUNDVEKT==0),])/nrow(d5)
```

## Excessive duration
percent trips longer than 3500 min
```{r}
nrow(d5[which(d5$VARIGHET>3500),])/nrow(d5)
```

## Export results
```{r, export, echo=TRUE}
#d5 <- subset(d5, d5$VARIGHET<3500)

d5 %>% filter(AKTIVITET=="I fiske" & REDSKAP_PROBLEMER=="Ingen") %>% saveRDS(., file=paste0("D:/Dropbox/IMR_projects/Data_call/Fisheries_data/data/processed/dat_", year, ".RDS"))
d5 %>% filter(REDSKAP_PROBLEMER=="Ingen") %>% saveRDS(., file=paste0("D:/Dropbox/IMR_projects/Data_call/Fisheries_data/data/processed/dat_", year, "_all.RDS"))

```
